{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入torch包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "import numpy as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试一下分割\n",
    "\n",
    "得出结论：**你需要先运行导包内容，后面段落的代码再运行时就不需要再导包了**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "['/Users/yingmuzhi/Desktop/artificial_intelligence/Learning/DL/section2', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python39.zip', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9/lib-dynload', '', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "print(sys.path)\n",
    "nn.abs(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1704e-41, 0.0000e+00, 2.2369e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([[0.1236, 0.8112, 0.1711],\n",
      "        [0.1610, 0.7297, 0.3009],\n",
      "        [0.1941, 0.9683, 0.3542],\n",
      "        [0.7760, 0.9145, 0.8681],\n",
      "        [0.4423, 0.5590, 0.3052]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.5763, -1.2797,  1.6908],\n",
      "        [-0.8850,  0.2311,  0.9754],\n",
      "        [ 0.9174, -1.4748,  0.5364],\n",
      "        [ 0.5297,  1.0145, -0.8939],\n",
      "        [-1.4179, -1.2042, -0.8497]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.2.1 创建Tensor\n",
    "'''\n",
    "# 创建5x3未初始化的Tensor\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "\n",
    "# 创建5x3随机初始化Tensor\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "# 创建5x3的long型全0的Tensor\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "\n",
    "# 直接根据数据创建\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "\n",
    "# 根据现有Tensor来创建\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)  # 指定新的数据类型\n",
    "print(x)\n",
    "\n",
    "# 获取Tensor的形状\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "\n",
    "# 官网上有很多种创建Tensor的方式，都可以在创建Tensor的时候指定数据类型dtype和存放device(cpu/gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.2.2 操作\n",
    "加法\n",
    "'''\n",
    "\n",
    "# 加法1\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "# 加法2\n",
    "print(torch.add(x, y))\n",
    "# 加法2 - 指定输出模式\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "# 加法3 - inplace\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([[-1.5763, -1.2797,  1.6908],\n",
      "        [-0.8850,  0.2311,  0.9754],\n",
      "        [ 0.9174, -1.4748,  0.5364],\n",
      "        [ 0.5297,  1.0145, -0.8939],\n",
      "        [-1.4179, -1.2042, -0.8497]])\n",
      "y is tensor([-1.5763, -1.2797,  1.6908])\n",
      "y' is tensor([-0.5763, -0.2797,  2.6908])\n",
      "x'[0, :] is tensor([-0.5763, -0.2797,  2.6908])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "索引\n",
    "'''\n",
    "\n",
    "# 使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：**若修改索引出来的部分数据，则原数据也会被修改，即共享内存**\n",
    "print(\"x is {}\".format(x))\n",
    "y = x[0, :]\n",
    "print(\"y is {}\".format(y))\n",
    "y += 1  # 数加，即给矩阵中每个元素加1\n",
    "print(\"y' is {}\\nx'[0, :] is {}\".format(y, x[0, :]))    # 源x也改变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5763, -0.2797,  2.6908],\n",
      "        [-0.8850,  0.2311,  0.9754],\n",
      "        [ 0.9174, -1.4748,  0.5364],\n",
      "        [ 0.5297,  1.0145, -0.8939],\n",
      "        [-1.4179, -1.2042, -0.8497]])\n",
      "torch.Size([5, 3])\n",
      "x's shape is torch.Size([5, 3]), y's size() is torch.Size([15]), and z's shape is torch.Size([3, 5])\n",
      "\n",
      "y is tensor([-0.5763, -0.2797,  2.6908, -0.8850,  0.2311,  0.9754,  0.9174, -1.4748,\n",
      "         0.5364,  0.5297,  1.0145, -0.8939, -1.4179, -1.2042, -0.8497])\n",
      "x' is tensor([[ 0.4237,  0.7203,  3.6908],\n",
      "        [ 0.1150,  1.2311,  1.9754],\n",
      "        [ 1.9174, -0.4748,  1.5364],\n",
      "        [ 1.5297,  2.0145,  0.1061],\n",
      "        [-0.4179, -0.2042,  0.1503]])\n",
      "y is tensor([ 0.4237,  0.7203,  3.6908,  0.1150,  1.2311,  1.9754,  1.9174, -0.4748,\n",
      "         1.5364,  1.5297,  2.0145,  0.1061, -0.4179, -0.2042,  0.1503])\n",
      "\n",
      "x_cp is tensor([ 0.4237,  0.7203,  3.6908,  0.1150,  1.2311,  1.9754,  1.9174, -0.4748,\n",
      "         1.5364,  1.5297,  2.0145,  0.1061, -0.4179, -0.2042,  0.1503])\n",
      "we have made some changes to x_cp, and x_cp now is tensor([ 0.4237,  0.7203,  3.6908,  0.1150,  1.2311,  1.9754,  1.9174, -0.4748,\n",
      "         1.5364,  1.5297,  2.0145,  0.1061, -0.4179, -0.2042,  0.1503])\n",
      "\n",
      "tensor([-0.5014])\n",
      "-0.5013514161109924\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "改变形状\n",
    "'''\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5)   # -1所指维度可以根据其他维度推算出来\n",
    "print(\"x's shape is {}, y's size() is {}, and z's shape is {}\".format(x.shape, y.shape, z.shape))\n",
    "\n",
    "# view()返回的新Tensor与源Tensor共享内存，改变其中一个，另一个也会改变\n",
    "print(\"\\ny is {}\".format(y))\n",
    "x += 1\n",
    "print(\"x' is {}\".format(x))\n",
    "print(\"y is {}\".format(y))\n",
    "\n",
    "# reshape()不推荐使用，虽然该函数可以改变形状，但是此函数并不能保证返回的是数据的拷贝\n",
    "\n",
    "# 为了完全拷贝数据，而不是使用索引（同一块内存），我们推荐用clone()创造副本后再用view()\n",
    "x_cp = x.clone().view(15)   # 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "print(\"\\nx_cp is {}\".format(x_cp))\n",
    "x -= 1\n",
    "print(\"we have made some changes to x_cp, and x_cp now is {}\".format(x_cp))\n",
    "\n",
    "# item()函数也常用，它可以将**标量Tensor**转换成一个Python number\n",
    "x = torch.randn(1)\n",
    "print(\"\\n{}\".format(x))\n",
    "print(x.item())\n",
    "# y = torch.randn(2, 2) # 只能对标量(0维)使用item()\n",
    "# print(y.item())\n",
    "\n",
    "# Pytorch 中的 Tensor 支持超过一百种操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([1, 2]), size is torch.Size([2])\n",
      "and y is tensor([[-0.2044, -0.1446,  0.9551]]), shape is torch.Size([1, 3])\n",
      "tensor([[ 0.9148,  0.2893, -0.3741],\n",
      "        [-1.3647,  0.9492,  0.6205],\n",
      "        [-0.7172, -0.2807, -1.6604]])\n",
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# arange()和randn()使用\n",
    "# tensor([])的类型一定如左侧，其中一定是([])元组tuple内一个列表list\n",
    "x = torch.arange(1, 3)  # 1, 2  start number, end number\n",
    "y = torch.randn(1, 3) # 1 x 3\n",
    "print(\"x is {}, size is {}\\nand y is {}, shape is {}\".format(x, x.shape, y, y.shape))\n",
    "z = torch.randn(3, 3)\n",
    "print(z)\n",
    "\n",
    "# 广播机制 --- 复制后再相加\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original y is 0x7fde84954b80, and new y is 0x7fde844b2450\n",
      "\n",
      "x's address is 0x7fde849541d0, z=x z's address is 0x7fde849541d0, w=x[:] w's address is 0x7fde84ba9540\n",
      "\n",
      "x origin is 0x7fde849541d0, and new x is 0x7fde849541d0\n"
     ]
    }
   ],
   "source": [
    "# 索引操作时不会开辟新内存的，而诸如y = x + y这样的加法运算会开辟新内存，将y指向新内存\n",
    "# 使用python内置id()函数判断内存地址。id()返回十进制内存地址\n",
    "# hex()将十进制转换为十六进制，hex(id())返回实际十六进制内存地址\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = x + y\n",
    "print(\"original y is {}, and new y is {}\".format(hex(id_before), hex(id(y))))\n",
    "\n",
    "# 测试变量内存地址，使用w = x[:] 则给w开辟新的内存空间\n",
    "z = x\n",
    "w = x[:]\n",
    "print(\"\\nx's address is {}, z=x z's address is {}, w=x[:] w's address is {}\\n\".format(hex(id(x)), hex(id(z)), hex(id(w))))\n",
    "\n",
    "# 使用索引做左值使得内存不变\n",
    "x_before = hex(id(x))\n",
    "x[:] = x + y\n",
    "print(\"x origin is {}, and new x is {}\".format(x_before, hex(id(x))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_cp39_PA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70eebdb8be5a84a025b99a5c636f96b3fa97906107ce3738272310b52f34377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
