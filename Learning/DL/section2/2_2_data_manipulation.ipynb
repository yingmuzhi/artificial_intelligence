{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入torch包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "import numpy as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试一下分割\n",
    "\n",
    "得出结论：**你需要先运行导包内容，后面段落的代码再运行时就不需要再导包了**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "['/Users/yingmuzhi/Desktop/artificial_intelligence/Learning/DL/section2', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python39.zip', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9/lib-dynload', '', '/Users/yingmuzhi/anaconda3/envs/env_cp39_PA/lib/python3.9/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "print(sys.path)\n",
    "nn.abs(-1)\n",
    "\n",
    "# 实验结束，开启学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节将介绍如何对内存中的数据进行操作。Pytorch的Tensor和NumPy的多维数组类似，而Tensor提供GPU计算和自动求梯度功能，Tensor更适合深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 0.0000e+00,  3.6893e+19, -2.8455e+24],\n",
      "        [-2.5250e-29,  1.1210e-44, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.4013e-45,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0.1123, 0.4316, 0.1209],\n",
      "        [0.9542, 0.1427, 0.2661],\n",
      "        [0.9075, 0.9757, 0.6445],\n",
      "        [0.8989, 0.8833, 0.2303],\n",
      "        [0.4268, 0.5302, 0.8775]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.6276,  0.5053,  1.6258],\n",
      "        [-0.2542, -1.5095,  0.0746],\n",
      "        [-0.6185,  0.7814, -0.1947],\n",
      "        [ 0.0511,  1.6160,  0.6769],\n",
      "        [-0.3482, -0.4944,  0.4832]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.2.1 创建Tensor\n",
    "'''\n",
    "# 用torch.arrage()创建1维Tensor\n",
    "z = torch.arange(1, 3)\n",
    "print(z)\n",
    "print(z.shape)\n",
    "z = torch.randn(2, 2)\n",
    "print(z.shape)  # tensor.shape和tensor.size()返回一个tensor([])，其中有几个数反应有几个维度，第一个数代表第一维度有几个元素，第二个数表示第二维度有几个数，以此类推\n",
    "\n",
    "# 创建5x3未初始化的Tensor\n",
    "x = torch.empty(5, 3)   # 返回5x3有未初始化数据的张量\n",
    "print(x)\n",
    "\n",
    "# 创建5x3随机初始化Tensor\n",
    "x = torch.rand(5, 3)    # 返回5x3随机初始化数据的张量\n",
    "print(x)\n",
    "\n",
    "# 创建5x3的long型全0的Tensor\n",
    "x = torch.zeros(5, 3, dtype=torch.long) # zero全0，dtype指定数据类型\n",
    "print(x)\n",
    "\n",
    "# 直接根据数据创建\n",
    "x = torch.tensor([5.5, 3])  # 直接创建tensor([])\n",
    "print(x)\n",
    "\n",
    "# 根据现有Tensor来创建\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)   # 创建新tensor，返回相同dtype, device\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)  # 创建新tensor，指定新的dtype\n",
    "print(x)\n",
    "\n",
    "# 获取Tensor的形状\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "\n",
    "# 官网上有很多种创建Tensor的方式，都可以在创建Tensor的时候指定数据类型dtype和存放device(cpu/gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n",
      "tensor([[-1.3064, -0.8150,  1.8010],\n",
      "        [-0.5182,  0.4842,  1.5001],\n",
      "        [ 1.4538, -0.8576,  1.2629],\n",
      "        [ 1.3750,  1.5134, -0.0258],\n",
      "        [-1.3871, -0.9962, -0.2059]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.2.2 操作\n",
    "加法\n",
    "'''\n",
    "\n",
    "# 加法1\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "# 加法2\n",
    "print(torch.add(x, y))\n",
    "# 加法2 - 指定输出模式\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "# 加法3 - inplace\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引\n",
    "\n",
    "torch.Tensor索引功能类似NumPy的索引功能，可以用索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is a 5x3 Tensor, and its format is: tensor([[ 1.1871,  1.7313, -0.0070],\n",
      "        [ 1.7568, -0.3271,  0.7576],\n",
      "        [ 0.9035, -1.3489, -0.1099],\n",
      "        [ 0.6087, -0.7997, -0.2086],\n",
      "        [-0.8527, -0.3053,  1.2808]])\n",
      "y is a 1x3 Tensor, and its format is: tensor([ 1.1871,  1.7313, -0.0070])\n",
      "y' is tensor([2.1871, 2.7313, 0.9930])\n",
      "x'[0, :] is tensor([2.1871, 2.7313, 0.9930])\n",
      "They all changed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "索引\n",
    "'''\n",
    "\n",
    "# 使用Tensor加中括号的方式进行缩影，即Tensor[]，第一个数指示第一个维度，第二个数指示第二个维度，以此类推...\n",
    "# 使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：**若修改索引出来的部分数据，则原数据也会被修改，即共享内存**\n",
    "x = torch.randn(5, 3)   # 两个维度，第一个维度大小为5，第二个维度大小为3\n",
    "print(\"x is a 5x3 Tensor, and its format is: {}\".format(x))\n",
    "y = x[0, :]             # 第一个维度指定为0，其余维度全访问\n",
    "print(\"y is a 1x3 Tensor, and its format is: {}\".format(y))\n",
    "y += 1  # 数加，即给矩阵中每个元素加1\n",
    "print(\"y' is {}\\nx'[0, :] is {}\\nThey all changed\".format(y, x[0, :]))    # 源x也改变了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 改变形状\n",
    "\n",
    "用`view()`来改变`Tensor`形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1871,  2.7313,  0.9930],\n",
      "        [ 1.7568, -0.3271,  0.7576],\n",
      "        [ 0.9035, -1.3489, -0.1099],\n",
      "        [ 0.6087, -0.7997, -0.2086],\n",
      "        [-0.8527, -0.3053,  1.2808]])\n",
      "torch.Size([5, 3])\n",
      "x's shape is torch.Size([5, 3]), y's size() is torch.Size([15]), and z's shape is torch.Size([3, 5])\n",
      "\n",
      "y is tensor([ 2.1871,  2.7313,  0.9930,  1.7568, -0.3271,  0.7576,  0.9035, -1.3489,\n",
      "        -0.1099,  0.6087, -0.7997, -0.2086, -0.8527, -0.3053,  1.2808])\n",
      "x' is tensor([[ 3.1871,  3.7313,  1.9930],\n",
      "        [ 2.7568,  0.6729,  1.7576],\n",
      "        [ 1.9035, -0.3489,  0.8901],\n",
      "        [ 1.6087,  0.2003,  0.7914],\n",
      "        [ 0.1473,  0.6947,  2.2808]])\n",
      "y is tensor([ 3.1871,  3.7313,  1.9930,  2.7568,  0.6729,  1.7576,  1.9035, -0.3489,\n",
      "         0.8901,  1.6087,  0.2003,  0.7914,  0.1473,  0.6947,  2.2808])\n",
      "\n",
      "x_cp is tensor([ 3.1871,  3.7313,  1.9930,  2.7568,  0.6729,  1.7576,  1.9035, -0.3489,\n",
      "         0.8901,  1.6087,  0.2003,  0.7914,  0.1473,  0.6947,  2.2808])\n",
      "we have made some changes to x_cp, and x_cp now is tensor([ 3.1871,  3.7313,  1.9930,  2.7568,  0.6729,  1.7576,  1.9035, -0.3489,\n",
      "         0.8901,  1.6087,  0.2003,  0.7914,  0.1473,  0.6947,  2.2808])\n",
      "\n",
      "tensor([[[[[0.5782]]]]])\n",
      "0.5781624913215637\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "改变形状\n",
    "'''\n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5)   # -1所指维度可以根据其他维度推算出来\n",
    "print(\"x's shape is {}, y's size() is {}, and z's shape is {}\".format(x.shape, y.shape, z.shape))\n",
    "\n",
    "# view只不过将Tensor展平\n",
    "# view()返回的新Tensor与源Tensor共享内存，改变其中一个，另一个也会改变\n",
    "print(\"\\ny is {}\".format(y))\n",
    "x += 1\n",
    "print(\"x' is {}\".format(x))\n",
    "print(\"y is {}\".format(y))\n",
    "\n",
    "# reshape()不推荐使用，虽然该函数可以改变形状，但是此函数并不能保证返回的是数据的拷贝\n",
    "\n",
    "# 为了完全拷贝数据，而不是使用索引（同一块内存），我们推荐用clone()创造副本后再用view()\n",
    "x_cp = x.clone().view(15)   # 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "print(\"\\nx_cp is {}\".format(x_cp))\n",
    "x -= 1\n",
    "print(\"we have made some changes to x_cp, and x_cp now is {}\".format(x_cp))\n",
    "\n",
    "# item()函数也常用，它可以将**不论多少维度，只要是单个一行一列的标量元素Tensor**转换成一个Python number\n",
    "x = torch.randn(1, 1, 1, 1, 1)  # 5维(5个中括号, 含最外层中括号一共5个)一个元素\n",
    "print(\"\\n{}\".format(x))\n",
    "print(x.item())\n",
    "# y = torch.randn(2, 2) # 只能对标量(0维)使用item()\n",
    "# print(y.item())\n",
    "\n",
    "# Pytorch 中的 Tensor 支持超过一百种操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([1, 2]), size is torch.Size([2])\n",
      "and y is tensor([[-0.2044, -0.1446,  0.9551]]), shape is torch.Size([1, 3])\n",
      "tensor([[ 0.9148,  0.2893, -0.3741],\n",
      "        [-1.3647,  0.9492,  0.6205],\n",
      "        [-0.7172, -0.2807, -1.6604]])\n",
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# arange()和randn()使用\n",
    "# tensor([])的类型一定如左侧，其中一定是([])元组tuple内一个列表list\n",
    "x = torch.arange(1, 3)  # 1, 2  start number, end number\n",
    "y = torch.randn(1, 3) # 1 x 3\n",
    "print(\"x is {}, size is {}\\nand y is {}, shape is {}\".format(x, x.shape, y, y.shape))\n",
    "z = torch.randn(3, 3)\n",
    "print(z)\n",
    "\n",
    "# 广播机制 --- 复制后再相加\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original y is 0x7fde84954b80, and new y is 0x7fde844b2450\n",
      "\n",
      "x's address is 0x7fde849541d0, z=x z's address is 0x7fde849541d0, w=x[:] w's address is 0x7fde84ba9540\n",
      "\n",
      "x origin is 0x7fde849541d0, and new x is 0x7fde849541d0\n"
     ]
    }
   ],
   "source": [
    "# 索引操作时不会开辟新内存的，而诸如y = x + y这样的加法运算会开辟新内存，将y指向新内存\n",
    "# 使用python内置id()函数判断内存地址。id()返回十进制内存地址\n",
    "# hex()将十进制转换为十六进制，hex(id())返回实际十六进制内存地址\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = x + y   # 加法赋值给普通变量会开辟新的内存空间\n",
    "print(\"original y is {}, and new y is {}\".format(hex(id_before), hex(id(y))))\n",
    "\n",
    "# 测试python变量内存地址，使用w = x[:] 则给w开辟新的内存空间\n",
    "z = x\n",
    "w = x[:]    # python规则，将索引赋值给普通变量会开辟新的内存空间，否则只是传指针\n",
    "print(\"\\nx's address is {}, z=x z's address is {}, w=x[:] w's address is {}\\n\".format(hex(id(x)), hex(id(z)), hex(id(w))))\n",
    "\n",
    "# 使用索引做左值使得内存不变\n",
    "x_before = hex(id(x))\n",
    "x[:] = x + y    # 维度和元素个数相同shape，加法赋值给索引变量则不会开辟新空间\n",
    "print(\"x origin is {}, and new x is {}\".format(x_before, hex(id(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Tensor 和 NumPy\n",
    "\n",
    "我们可以使用`numpy()`和`from_numpy()`将Tensor和NumPy中的数组相互转换。但是需要注意的是：该函数产生的**Tensor和NumPy中的数组共享内存，所以转换得快**。\n",
    "\n",
    "torch.tensor()会将NumPy数据拷贝为tensor，不再共享内存。\n",
    "\n",
    "#### Tensor 转 NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.]])\t\t[[1. 1. 1. 1. 1.]]\n",
      "tensor([[2., 2., 2., 2., 2.]])\t\t[[2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tensor 转 NumPy\n",
    "\n",
    "使用`numpy()`将`Tensor`转换为NumPy数组\n",
    "'''\n",
    "\n",
    "c = torch.ones(5)       # 一维 只有第一维度有5个元素    tensor([]) - 有几个维度，最左边就有几个中括号，最少为1维\n",
    "a = torch.ones(1, 5)    # 二维 1x5\n",
    "b = a.numpy()\n",
    "print(a, b, sep=\"\\t\\t\")\n",
    "\n",
    "a += 1\n",
    "print(a, b, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy 转 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\t\ttensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''from_numpy()'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b, sep=\"\\t\\t\")\n",
    "\n",
    "c = torch.tensor(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Tensor on GPU\n",
    "\n",
    "用方法`to()`可以实现CPU和GPU间移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry, there is no nvidia-GPU\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n",
    "else:\n",
    "    print(\"sorry, there is no nvidia-GPU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_cp39_PA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70eebdb8be5a84a025b99a5c636f96b3fa97906107ce3738272310b52f34377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
